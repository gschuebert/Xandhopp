# Xandhopp Data Pipeline

## üåç √úbersicht

Das Xandhopp Data Pipeline System ist eine professionelle, skalierbare Architektur f√ºr die Sammlung, Normalisierung und Bereitstellung von L√§nderinformationen aus verschiedenen √∂ffentlichen und kommerziellen APIs.

## üèóÔ∏è Architektur

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Sources  ‚îÇ    ‚îÇ  Ingestion Layer ‚îÇ    ‚îÇ  Storage Layer  ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ World Bank    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  BullMQ Worker   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ClickHouse     ‚îÇ
‚îÇ ‚Ä¢ US State Dept ‚îÇ    ‚îÇ  ‚Ä¢ HTTP Client   ‚îÇ    ‚îÇ  ‚Ä¢ Indicators   ‚îÇ
‚îÇ ‚Ä¢ FCDO (UK)     ‚îÇ    ‚îÇ  ‚Ä¢ Rate Limiting ‚îÇ    ‚îÇ  ‚Ä¢ Advisories   ‚îÇ
‚îÇ ‚Ä¢ OpenAQ        ‚îÇ    ‚îÇ  ‚Ä¢ Retry Logic   ‚îÇ    ‚îÇ  ‚Ä¢ Air Quality  ‚îÇ
‚îÇ ‚Ä¢ WHO/GHO       ‚îÇ    ‚îÇ  ‚Ä¢ Scheduling    ‚îÇ    ‚îÇ  ‚Ä¢ Cost Living  ‚îÇ
‚îÇ ‚Ä¢ OECD          ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ                        ‚îÇ
                                ‚ñº                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   API Layer     ‚îÇ    ‚îÇ   Redis Queue    ‚îÇ    ‚îÇ   PostgreSQL    ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ REST APIs     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  ‚Ä¢ Job Queue     ‚îÇ    ‚îÇ  ‚Ä¢ Core Entities‚îÇ
‚îÇ ‚Ä¢ GraphQL       ‚îÇ    ‚îÇ  ‚Ä¢ Scheduling    ‚îÇ    ‚îÇ  ‚Ä¢ Countries    ‚îÇ
‚îÇ ‚Ä¢ Health Checks ‚îÇ    ‚îÇ  ‚Ä¢ Monitoring    ‚îÇ    ‚îÇ  ‚Ä¢ Programs     ‚îÇ
‚îÇ ‚Ä¢ Rate Limiting ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ  ‚Ä¢ Providers    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìä Datenquellen

### üè¶ Economic & Social Indicators
- **World Bank**: GDP, Inflation, Arbeitslosigkeit, Lebenserwartung
- **OECD**: SDMX-basierte Indikatoren
- **WHO/GHO**: Gesundheitsindikatoren

### üõ°Ô∏è Travel & Security
- **US State Department**: Reisewarnungen (Level 1-4)
- **FCDO (UK)**: Britische Reisehinweise
- **Optional**: GDACS (Katastrophenwarnungen)

### üå¨Ô∏è Environmental Data
- **OpenAQ**: Luftqualit√§tsmessungen (PM2.5, PM10, NO2, O3)
- **Optional**: Climate APIs

### üí∞ Cost of Living
- **Numbeo**: Lebenshaltungskosten (API Key erforderlich)
- **TradingEconomics**: Wirtschaftsdaten (API Key erforderlich)

## üöÄ Quick Start

### 1. Setup ausf√ºhren
```bash
# Clone und Dependencies installieren
pnpm install

# Environment konfigurieren
cp .env.example .env.local
# Bearbeite .env.local mit deinen Einstellungen

# Komplettes Setup (Infrastructure + Schema + Apps)
.\scripts\setup-data-pipeline.ps1
```

### 2. Manueller Start
```bash
# Infrastructure Services starten
pnpm run docker:up

# ClickHouse Schema anwenden
pnpm run ch:setup

# Ingestion Worker starten
pnpm run ingestion:dev

# Web & Admin Apps starten
pnpm run dev
```

### 3. System testen
```bash
# Vollst√§ndiger System-Test
.\scripts\test-data-pipeline.ps1

# Einzelne Endpoints testen
pnpm run data:health
pnpm run data:countries
pnpm run data:snapshot
```

## üì° API Endpoints

### Health & Status
```http
GET /api/health
# Response: { status: "healthy", services: {...} }
```

### Countries
```http
GET /api/countries
# Response: { countries: ["DE", "ES", ...], total: 30 }
```

### Country Data
```http
# Vollst√§ndiger Snapshot
GET /api/country/{iso2}/snapshot
# Response: { advisory: {...}, indicators: [...], air_quality: [...] }

# Nur Indikatoren
GET /api/country/{iso2}/indicators?codes=NY.GDP.PCAP.KD,SP.DYN.LE00.IN

# Nur Luftqualit√§t
GET /api/country/{iso2}/airquality?parameters=pm25,no2&hours=24
```

## üóÑÔ∏è Datenbank Schema

### ClickHouse Tables

#### `portalis.indicators`
```sql
CREATE TABLE portalis.indicators (
    country_iso2 FixedString(2),
    source LowCardinality(String),
    indicator_code LowCardinality(String),
    period Date,
    value Nullable(Float64),
    meta JSON,
    ingested_at DateTime DEFAULT now()
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(period)
ORDER BY (country_iso2, indicator_code, period);
```

#### `portalis.advisories`
```sql
CREATE TABLE portalis.advisories (
    country_iso2 FixedString(2),
    source LowCardinality(String),
    level UInt8,
    headline String,
    url String,
    published_at DateTime,
    payload JSON,
    ingested_at DateTime DEFAULT now()
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(published_at)
ORDER BY (country_iso2, published_at);
```

#### `portalis.air_quality`
```sql
CREATE TABLE portalis.air_quality (
    country_iso2 FixedString(2),
    city String,
    parameter LowCardinality(String),
    ts DateTime,
    value Nullable(Float64),
    unit LowCardinality(String),
    source LowCardinality(String),
    ingested_at DateTime DEFAULT now()
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(ts)
ORDER BY (country_iso2, city, parameter, ts);
```

## ‚öôÔ∏è Konfiguration

### Environment Variables (.env.local)
```bash
# ClickHouse
CLICKHOUSE_HTTP=http://localhost:8123
CLICKHOUSE_DATABASE=portalis

# Redis (BullMQ)
REDIS_HOST=localhost
REDIS_PORT=6379

# API Keys (Optional)
OPENAQ_API_KEY=your_key_here
NUMBEO_API_KEY=your_key_here
TRADING_ECONOMICS_KEY=your_key_here

# Scheduling (Milliseconds)
ADVISORIES_INTERVAL=21600000    # 6 hours
INDICATORS_INTERVAL=86400000    # 24 hours
AIR_QUALITY_INTERVAL=43200000   # 12 hours

# Countries to monitor
MONITOR_COUNTRIES=DE,ES,PT,US,GB,FR,IT,NL,BE,AT
```

### Job Scheduling
- **Travel Advisories**: Alle 6 Stunden
- **Economic Indicators**: T√§glich
- **Air Quality**: Alle 12 Stunden

## üîß Development

### Package Structure
```
packages/
‚îú‚îÄ‚îÄ connectors/          # API Connectors & HTTP Clients
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ http.ts      # Shared HTTP utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ worldbank.ts # World Bank API
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stateDept.ts # US State Department
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fcdo.ts      # UK Foreign Office
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openaq.ts    # Air Quality API
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts     # Exports
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ db-clickhouse/       # ClickHouse Schema
‚îÇ   ‚îî‚îÄ‚îÄ schema.sql
‚îî‚îÄ‚îÄ ...

apps/
‚îú‚îÄ‚îÄ ingestion-worker/    # BullMQ Job Worker
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.ts     # Main worker
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.ts    # Configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.ts  # ClickHouse service
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ jobs/        # Job processors
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ web/                 # Next.js Web App
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lib/clickhouse.ts    # ClickHouse client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pages/api/           # API routes
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ ...
```

### Neue Datenquellen hinzuf√ºgen

1. **Connector erstellen**:
```typescript
// packages/connectors/src/myapi.ts
export async function fetchMyAPI(country: string) {
  const response = await httpGetJson(`https://api.example.com/data/${country}`);
  return response.data.map(item => ({
    country_iso2: country.toUpperCase(),
    source: "myapi",
    // ... normalize data
  }));
}
```

2. **Job Processor hinzuf√ºgen**:
```typescript
// apps/ingestion-worker/src/jobs/myapi.ts
export async function processMyAPI(job: Job, clickhouse: ClickHouseService) {
  const data = await fetchMyAPI(job.data.country);
  await clickhouse.insertIndicators(data);
}
```

3. **Scheduling konfigurieren**:
```typescript
// apps/ingestion-worker/src/index.ts
await queue.add("myapi", { country: "DE" }, { 
  repeat: { every: 60000 } 
});
```

## üö® Monitoring & Debugging

### Logs
```bash
# Ingestion Worker Logs
docker-compose logs -f ingestion-worker

# ClickHouse Logs
docker-compose logs -f clickhouse

# All services
pnpm run docker:logs
```

### Health Checks
```bash
# System Health
curl http://localhost:3000/api/health

# ClickHouse Direct
curl http://localhost:8123/ping

# Redis
docker-compose exec redis redis-cli ping
```

### Performance Monitoring
```sql
-- ClickHouse Query Performance
SELECT query, query_duration_ms, memory_usage
FROM system.query_log 
WHERE type = 'QueryFinish'
ORDER BY event_time DESC LIMIT 10;

-- Data Volume
SELECT 
  table, 
  formatReadableSize(sum(bytes_on_disk)) as size,
  sum(rows) as rows
FROM system.parts 
WHERE database = 'portalis'
GROUP BY table;
```

## üîí Security & Rate Limiting

### API Rate Limits
- **World Bank**: 120 requests/minute
- **OpenAQ**: 2000 requests/hour (ohne API Key)
- **US State Dept**: Keine offiziellen Limits
- **FCDO**: H√∂fliche Nutzung empfohlen

### Retry Strategy
- **Initial Delay**: 500ms
- **Max Retries**: 3
- **Backoff**: Exponential
- **Rate Limit Handling**: Automatic delay

## üìà Skalierung

### Horizontal Scaling
```yaml
# docker-compose.yml
ingestion-worker:
  deploy:
    replicas: 3
  environment:
    - WORKER_CONCURRENCY=2
```

### ClickHouse Optimierung
```sql
-- Materialized Views f√ºr h√§ufige Abfragen
CREATE MATERIALIZED VIEW country_summary_mv AS
SELECT 
  country_iso2,
  argMax(value, period) as latest_gdp
FROM indicators 
WHERE indicator_code = 'NY.GDP.PCAP.KD'
GROUP BY country_iso2;
```

## üêõ Troubleshooting

### H√§ufige Probleme

1. **ClickHouse Connection Failed**
   ```bash
   # Container Status pr√ºfen
   docker-compose ps clickhouse
   
   # Logs pr√ºfen
   docker-compose logs clickhouse
   
   # Schema erneut anwenden
   pnpm run ch:schema
   ```

2. **Keine Daten in APIs**
   ```bash
   # Worker Status pr√ºfen
   curl http://localhost:3000/api/health
   
   # Job Queue Status (BullMQ Dashboard)
   # Oder Worker Logs pr√ºfen
   ```

3. **Rate Limiting**
   ```bash
   # API Keys in .env.local setzen
   OPENAQ_API_KEY=your_key
   NUMBEO_API_KEY=your_key
   ```

## üìö N√ºtzliche Ressourcen

- [World Bank API Docs](https://datahelpdesk.worldbank.org/knowledgebase/articles/889392)
- [OpenAQ API v3](https://docs.openaq.org/reference)
- [US State Dept Travel Advisories](https://travel.state.gov/content/travel/en/traveladvisories.html)
- [ClickHouse Documentation](https://clickhouse.com/docs)
- [BullMQ Documentation](https://docs.bullmq.io/)

## üéØ N√§chste Schritte

1. **Zus√§tzliche Datenquellen**:
   - WHO Global Health Observatory
   - OECD SDMX APIs
   - UN SDG Indicators
   - Climate APIs

2. **Erweiterte Features**:
   - GraphQL API
   - Real-time Updates via WebSockets
   - Data Validation & Quality Checks
   - Automated Alerts

3. **Performance Optimierung**:
   - ClickHouse Cluster Setup
   - Caching Layer (Redis)
   - CDN f√ºr API Responses

4. **Monitoring & Observability**:
   - Prometheus Metrics
   - Grafana Dashboards
   - OpenTelemetry Tracing
